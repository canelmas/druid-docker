druid.service=druid/historical
druid.plaintextPort=8083

# HTTP server threads
druid.server.http.numThreads=10

# Processing threads and buffers
druid.processing.buffer.sizeBytes=1000000000

# The number of processing threads to have available for parallel processing of segments. 
# Our rule of thumb is num_cores - 1, which means that even under heavy load there will still be
# one core available to do background tasks like talking with ZooKeeper and pulling down segments.
# If only one core is available, this property defaults to the value 1.
# 
#  default = Number of cores - 1
# 
# druid.processing.numThreads=6

# The number of direct memory buffers available for merging query results. 
# The buffers are sized by druid.processing.buffer.sizeBytes. This property is 
# effectively a concurrency limit for queries that require merging buffers. 
# If you are using any queries that require merge buffers (currently, just groupBy v2) 
# then you should have at least two of these.
#
# default = max(2, druid.processing.numThreads / 4)
#
# druid.processing.numMergeBuffers=2

# Segment storage - 800 GB
druid.segmentCache.locations=[{"path":"var/druid/segment-cache","maxSize":800000000000}]
druid.server.maxSize=800000000000

# Query cache - 5 GB
druid.historical.cache.useCache=true
druid.historical.cache.populateCache=true
druid.cache.type=caffeine
druid.cache.sizeInBytes=5000000000

